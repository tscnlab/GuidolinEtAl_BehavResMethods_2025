---
title: "metrics_stats"
author: "Carolina Guidolin"
date: "2024-12-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Aim

To calculate differences between metrics calculated in the script 06_metrics_comparison.Rmd. Specifically, we will be using a t-test to do this.

## Let's first check the normality of our data

### Import the norm_check functions before running this as well as necessary packages

```{r}
# Function to check normality
base::source(here::here("03_analysis", "functions", "norm_check_funs.R"))

library(ggpubr)
library(rstatix)
library(tidyverse)
```
### Importing metrics dfs if they are not already in global environment
```{r}
metric_files <- list.files(
  path = here::here("outputs"), # choose directory based on where data was saved
  pattern = "\\.csv$",
  full.names = TRUE
)

# Loop through each file and assign to the global environment
for (file in metric_files) {
  df_name <- tools::file_path_sans_ext(basename(file))
  assign(
    df_name,
    readr::read_csv(file, show_col_types = FALSE),
    envir = .GlobalEnv
  )
}
```

### Testing for normality of residuals

```{r}
options(scipen = 999) # avoid scientific notation 
set.seed(20250709)

# Create list of data frames containing metric values
light_metrics_list <- list(
  mlit250_all = mlit250_all,
  mlit1000_all = mlit1000_all,
  llit10_all = llit10_all,
  llit250_all = llit250_all,
  llit1000_all = llit1000_all,
  flit10_all = flit10_all,
  flit250_all = flit250_all,
  flit1000_all = flit1000_all,
  tat1000_all = tat1000_all,
  tat250_all = tat250_all,
  mean_all = mean_all,
  m10_all = m10_all,
  iv_all = iv_all,
  is_all = is_all
)

# Visualise normality of the means for each df
for (i in seq_along(light_metrics_list)) {
  df <- light_metrics_list[[i]]
  df_name <- names(light_metrics_list)[i]
  print(norm_check_means(df, df_name = df_name))
}


## The distribution does not look normal, and for several metrics the Shapiro Wilk test also returns significant results
## Hence, non-parametric alternative is needed

```

### Performing the non-parametric Friedman test for the metrics, and calculate pairwise comparisons (post-hoc tests) using Wilcoxon test

The first step is some renaming of columns across datasets, i.e. changing cols to "raw", "clusters", "wrlg" and eliminating the "_all" from the metric
```{r}
# Keep copy (need for later)
light_metrics_cleaned <- lapply(light_metrics_list, function(df) {
  # Make a copy of the dataframe
  df_new <- df

  # Rename mean, IV, and IS columns
  names(df_new) <- sub("^mean_raw$", "raw", names(df_new))
  names(df_new) <- sub("^mean_wrlg$", "wrlg", names(df_new))
  names(df_new) <- sub("^mean_clusters$", "clusters", names(df_new))

  names(df_new) <- sub("^IV_raw$", "raw", names(df_new))
  names(df_new) <- sub("^IV_wrlg$", "wrlg", names(df_new))
  names(df_new) <- sub("^IV_clusters$", "clusters", names(df_new))
  
  names(df_new) <- sub("^IS_raw$", "raw", names(df_new))
  names(df_new) <- sub("^IS_wrlg$", "wrlg", names(df_new))
  names(df_new) <- sub("^IS_clusters$", "clusters", names(df_new))

  # Remove 'delta_' and 'sd_' columns
  df_new <- df_new[ , !grepl("^delta_|^sd_", names(df_new)) ]

  return(df_new)
})
```


#### Function to perform the two test for all metrics

```{r}
# Friedman test
## First, we create a function for the metrics that have been calculated as average of 6 participation days
## These are: mlit250, mlit1000, llit10, llit250, llit1000, flit10, flit250, flit1000, tat1000, tat250, mean (daytime, M10m)

friedman_and_posthoc <- function(df, pval_threshold = 0.05, metric_name = NULL) {
  
  # Get the name of the input df as a string
    if (!is.null(metric_name)) {
      metric_name <- sub("_all$", "", metric_name) # removes _all from metric name
      } else {
        metric_name <- "unknown_metric"
        }
  
  # Turn into long format
  df_long <- df %>%
    pivot_longer(
      cols = c("raw", "wrlg", "clusters"),
      names_to = "dataset",
      values_to = "value"
    ) %>%
    mutate(value = as.numeric(value),
           Id = as.factor(Id),
           dataset = factor(dataset, levels = c("raw", "wrlg", "clusters")) # order the levels, makes the post-hoc comparisons easier to interpret
           ) %>%
    ungroup()
  
  summary(df_long$value)
  
  # Friedman test and effect size (Kendall's W)
  friedman <- df_long %>%
    rstatix::friedman_test(value ~ dataset | Id) %>%
    mutate(
      kendallsw = rstatix::friedman_effsize(df_long, value ~ dataset | Id)$effsize,
      metric = metric_name
    ) %>%
    add_significance("p") # to show *
  
  # Post-hoc only if friedman test is significant
  if (friedman$p < pval_threshold) {
    # Perform Wilcoxon pairwise comparisons
    posthoc <- df_long %>%
      rstatix::pairwise_wilcox_test(
        value ~ dataset,
        paired = TRUE,
        p.adjust.method = "fdr"
      ) %>%
      # Add metric name
      mutate(
        metric = metric_name # add metric name
    )
  } 
  else {
  posthoc <- NULL
  }
  
return(list(
    friedman = friedman,
    posthoc = posthoc
  ))
  
}

```

### Apply Friedman testing function to all metrics dfs

```{r}
library(purrr)

# Use list of light metrics indicated above

results <- lapply(names(light_metrics_cleaned), function(nm) {
  fried_res <- friedman_and_posthoc(light_metrics_cleaned[[nm]], 0.05, nm)
  return(fried_res)
})

# Save results in dfs
friedman_all <- purrr::map_dfr(results, "friedman")
posthoc_all  <- purrr::map_dfr(results, "posthoc") 

# Rename some 
```

### Performing equivalence testing

The main idea of performing equivalence testing here is to compare the metrics across datasets and identify whether the difference is small enough to treat the three datasets as practically the same. We perform equivalence testing using bootstrapping. Bootstrapping is a way to get an empirical distribution of possible effect sizes (as if we repeated the study many times), i.e. estimating the sampling distribution of our effect size. So put simply, bootstrapping gives us a way to answer: How consistent (robust) is my evidence that this difference is small enough to be ignorable?

### Applying bootstrap eq testing

```{r}

# First, rename the metric names in light_metric_list so that do not contain _all after the metric name
names(light_metrics_cleaned) <- sub("_all$", "", names(light_metrics_cleaned))

run_bootstrap_equiv_test <- function(metric_name, group1, group2, light_metrics_cleaned,
                                     n_bootstraps = 10^4, SESOI = NULL, alpha = 0.05) {
  
  # Pull the original data
  df <- light_metrics_cleaned[[metric_name]]
  if (is.null(df)) stop(paste0("Metric '", metric_name, "' not found!"))
  
  data1 <- as.numeric(df[[group1]])
  data2 <- as.numeric(df[[group2]])
  
  # Checking if cols are correctly identified 
  if (any(is.null(c(data1, data2)))) {
    return(tibble::tibble(
      error = paste0("Group columns not found: ", group1, ", ", group2),
      cohens_d = NA_real_,
      lower_CI = NA_real_,
      upper_CI = NA_real_,
      is_equivalent = NA,
      prop_inside_SESOI = NA_real_,
      prop_outside_SESOI = NA_real_
    ))
  }
  
  # Calculate effect size and bootstraps
  effect.size <- effectsize::cohens_d(data1, data2, paired = TRUE)$Cohens_d
  #Apply costum function
  bootstraps <- bootstrap_differences(n_bootstraps, data1, data2)
  
  # Filtering non-NA bootstraps 
  bootstraps <- bootstraps[!is.na(bootstraps)]
  
  if (length(bootstraps) == 0) {
    return(tibble::tibble(
      error = "All bootstrap samples are NA",
      cohens_d = effect.size,
      lower_CI = NA_real_,
      upper_CI = NA_real_,
      is_equivalent = NA,
      prop_inside_SESOI = NA_real_,
      prop_outside_SESOI = NA_real_
    ))
  }
  
  # Calculate proportion inside and outside of SESOI
  prop_inside_SESOI <- sum(bootstraps >= -SESOI & bootstraps <= SESOI, na.rm = TRUE)/
                  length(bootstraps)
  prop_outside_SESOI <- sum(bootstraps < -SESOI | bootstraps > SESOI, na.rm = TRUE)/
                  length(bootstraps)

  # Visualise the bootstraps
  ci <- quantile(bootstraps, probs = c(0.05, 0.95))
  lower_CI <- ci[1]
  upper_CI <- ci[2]
  
  plot_obj <- tibble::tibble(value = bootstraps) %>%
    mutate(outside_SESOI = factor(value >= SESOI | value <= -SESOI,
                                  levels = c(FALSE, TRUE)
                                  )
                             ) %>%
  ggplot(aes(x = value,
             fill = outside_SESOI)) +
  geom_histogram(binwidth = 0.02)+
  coord_cartesian(xlim = c(-1.5, 1.5)) +
  labs(
    title = paste0("Metric: ", metric_name, "\n", group1, " vs ", group2),
    #subtitle = paste0("Smallest effect size of interest (SESOI):", SESOI),
    x = "Effect size",
    fill = "Effect outside SESOI"
  ) +
  # Add SESOI region, CIs and observed eff size 
  geom_vline(
    aes(xintercept = effect.size, color = "Observed Cohen's d"),
  linetype = "dashed",
  linewidth = 0.7
  ) +
  geom_vline(
    aes(xintercept = lower_CI, color = "CI bounds"),
  linetype = "dashed",
  linewidth = 0.7
  ) +
  geom_vline(
    aes(xintercept = upper_CI, color = "CI bounds"),
    linetype = "dashed",
    linewidth = 0.7
  ) +
  # Shaded SESOI region
  annotate(
    "rect",
    xmin = -SESOI, xmax = SESOI,
    ymin = 0, ymax = Inf,
    fill = "grey", alpha = 0.2
  ) +
  scale_fill_manual(
    values = c(
      "FALSE" = "lightblue",   
      "TRUE" = "orange"),
    drop = FALSE) +
  scale_color_manual(
    name = "Colour",
    values = c(
      "Observed Cohen's d" = "red",
      "CI bounds" = "blue"
    )
  ) +
  theme_bw() 

  print(plot_obj)
  
    # Save plot with dynamic name
  ggsave(
  paste0(
    "bootstrap_",
    metric_name, "_",
    group1, "_vs_", group2, ".png"
  ),
  plot = plot_obj,
  path = here::here("equivalence_test_analysis", "outputs", "bootstraps"),
  width = 8,
  height = 5,
  bg = "white",
  dpi = 600
)

  
  # Calculating confidence intervals and classifying results 
  ci <- quantile(bootstraps, probs = c(alpha, 1 - alpha), na.rm = TRUE)
  eq_decision <- if ((ci[1] >= -SESOI) && (ci[2] <= SESOI)) {
    "equivalence"
  } else if ((ci[1] < -SESOI) && (ci[2] < -SESOI)) {
    "negative effect"
  } else if ((ci[1] > SESOI) && (ci[2] > SESOI)) {
    "positive effect"
  } else {
    "inconclusive"
  }
    
  
  tibble::tibble(
    cohens_d = effect.size,
    lower_CI = ci[1],
    upper_CI = ci[2],
    equivalence = eq_decision,
    prop_inside_SESOI = prop_inside_SESOI,
    prop_outside_SESOI = prop_outside_SESOI,
    SESOI = SESOI, 
    plot_obj = list(plot_obj) # needed for plotting later on 
  )
}

```


## Try out on all pairwise combinations from the post-hoc tests
Interpretation of the results
- If both ends of the CI lie within SESOI bounds: equivalence
- If any part of the CI extends outside SESOI, canâ€™t conclude equivalence -> inconclusive
- If both CIs are below SESOI: meaningfully negative effect
- Both CIs are above SESOI: meaningfully positive effect

```{r}
# Import boottrapping function
base::source(here::here("03_analysis", "functions", "bootstrapping_fun.R"))
```

```{r}
# Adding equivalence to post-hoc results 
equiv_results <- posthoc_all %>%
  mutate(equiv_result = pmap(
    list(metric, group1, group2),
    # For each row, run bootstrap eq test on pairs of groups for given metric using original data in metrics_list 
    ~ run_bootstrap_equiv_test(..1, ..2, ..3, SESOI = 0.3, light_metrics_cleaned)
  )) %>%
  tidyr::unnest(equiv_result)

```
#### Creating multiplot for bootstraps distributions
```{r}
library(patchwork)
library(grid)

# Extract all the plot objects
all_plots <- equiv_results$plot_obj

# Combine and collect guide (legend) to the bottom
combined_plot <- wrap_plots(all_plots) +
  plot_layout(guides = "collect") 
    

combined_plot_final <- combined_plot + 
  plot_annotation(
    title = "Bootstrap distributions for pairwise comparisons",
    subtitle = "SESOI = 0.3",
    theme = theme(plot.title = element_text(size=20),
                  plot.subtitle = element_text(size=14),
                  legend.position = "right",
                  legend.direction = "vertical") # Make the legend stack vertically
  )

ggsave(
  filename = "combined_bootstrap_plots.png",
  plot = combined_plot_final,
  width = 14,
  height = 13,
  dpi = 600,
  path = here::here("equivalence_test_analysis", "outputs", "bootstraps"),
  bg = "white"
)

```


### Visualising how the proportion inside/outside the SESOI varies based on SESOI values
First, slightly modify the function 
```{r}
# Same function but without the plot being created, and saving the sesoi
noplot_bootstrap_equiv_test <- function(metric_name, group1, group2, light_metrics_cleaned,
                                     n_bootstraps = 10^4, SESOI = NULL, alpha = 0.05) {
  
  # Pull the original data
  df <- light_metrics_cleaned[[metric_name]]
  if (is.null(df)) stop(paste0("Metric '", metric_name, "' not found!"))
  
  data1 <- as.numeric(df[[group1]])
  data2 <- as.numeric(df[[group2]])
  
  # Check columns
  if (any(is.null(c(data1, data2)))) {
    return(tibble::tibble(
      error = paste0("Group columns not found: ", group1, ", ", group2),
      cohens_d = NA_real_,
      lower_CI = NA_real_,
      upper_CI = NA_real_,
      is_equivalent = NA,
      prop_inside_SESOI = NA_real_,
      prop_outside_SESOI = NA_real_,
      SESOI = SESOI
    ))
  }
  
  # Calculate effect size and bootstraps
  effect.size <- effectsize::cohens_d(data1, data2, paired = TRUE)$Cohens_d
  bootstraps <- bootstrap_differences(n_bootstraps, data1, data2)
  bootstraps <- bootstraps[!is.na(bootstraps)]
  
  if (length(bootstraps) == 0) {
    return(tibble::tibble(
      error = "All bootstrap samples are NA",
      cohens_d = effect.size,
      lower_CI = NA_real_,
      upper_CI = NA_real_,
      is_equivalent = NA,
      prop_inside_SESOI = NA_real_,
      prop_outside_SESOI = NA_real_,
      SESOI = SESOI
    ))
  }
  
  # Calculate proportion inside and outside of SESOI
  prop_inside_SESOI <- sum(bootstraps >= -SESOI & bootstraps <= SESOI, na.rm = TRUE)/
                  length(bootstraps)
  prop_outside_SESOI <- sum(bootstraps < -SESOI | bootstraps > SESOI, na.rm = TRUE)/
                  length(bootstraps)
  
  # Calculating confidence intervals and classifying results 
  ci <- quantile(bootstraps, probs = c(alpha, 1 - alpha), na.rm = TRUE)
  eq_decision <- if ((ci[1] >= -SESOI) && (ci[2] <= SESOI)) {
    "equivalence"
  } else if ((ci[1] < -SESOI) && (ci[2] < -SESOI)) {
    "negative effect"
  } else if ((ci[1] > SESOI) && (ci[2] > SESOI)) {
    "positive effect"
  } else {
    "inconclusive"
  }
  
  tibble::tibble(
    cohens_d = effect.size,
    lower_CI = ci[1],
    upper_CI = ci[2],
    is_equivalent = eq_decision,
    prop_inside_SESOI = prop_inside_SESOI,
    prop_outside_SESOI = prop_outside_SESOI,
    SESOI = SESOI
  )
}

sesoi_seq <- seq(0, 1, by = 0.1)

# Empty tibble to store results
sesoi_sweep_all <- tibble(
  metric = character(),
  group1 = character(),
  group2 = character(),
  SESOI = numeric(),
  prop_outside_SESOI = numeric(),
  equivalence = character()
)

# Loop over posthoc_all rows and SESOI values
for (i in seq_len(nrow(posthoc_all))) {
  
  metric_name <- posthoc_all$metric[i]
  group1 <- posthoc_all$group1[i]
  group2 <- posthoc_all$group2[i]
  
  for (s in sesoi_seq) {
    
    res <- noplot_bootstrap_equiv_test(
      metric_name = metric_name,
      group1 = group1,
      group2 = group2,
      light_metrics_cleaned = light_metrics_cleaned,
      SESOI = s
    )
    
    sesoi_sweep_all <- bind_rows(
      sesoi_sweep_all,
      tibble(
        metric = metric_name,
        group1 = group1,
        group2 = group2,
        SESOI = s,
        prop_outside_SESOI = res$prop_outside_SESOI,
        equivalence = res$is_equivalent
      )
    )
  }
}

print(sesoi_sweep_all)
```

### Create individual plots as well as multiplot 
```{r}
# Create plots for each unique metric + group pair
plots <- sesoi_sweep_all %>%
  group_by(metric, group1, group2) %>%
  group_split() %>%
  map(\(df) {
    metric_name <- unique(df$metric)
    g1 <- unique(df$group1)
    g2 <- unique(df$group2)
    
    p <- ggplot(df, aes(x = SESOI, y = prop_outside_SESOI)) +
      geom_line() +
      geom_point(size = 1) +
      scale_y_continuous(breaks = c(0.0, 0.25, 0.50, 0.75, 1.0),
                         labels = c("0.0", "0.25", "0.50", "0.75", "1.0"),
                         limits = c(0, 1)
                         ) +
      scale_x_continuous(breaks = c(0.0, 0.25, 0.50, 0.75, 1.0),
                         labels = c("0.0", "0.25", "0.50", "0.75", "1.0"),
                         limits = c(0, 1)
                         ) +
      labs(
        title = paste0("Proportion outside SESOI\nMetric: ", metric_name,
                       "\n", g1, " vs ", g2),
        x = "SESOI",
        y = "Proportion outside SESOI"
      ) +
      theme_bw() 
    
    
    # Save
    ggsave(
      filename = paste0("prop_outside_", metric_name, "_", g1, "_vs_", g2, ".png"),
      plot = p,
      path = here::here("equivalence_test_analysis", "outputs", "sesoi"),
      width = 6,
      height = 4,
      dpi = 300,
      bg = "white"
    )
    
    p
  })

## 
# Create a new combined facet label
sesoi_sweep_faceted <- sesoi_sweep_all %>%
  mutate(facet_label = paste0("Metric: ", metric, "\n", group1, " vs ", group2))

sesoi_multiplot <- ggplot(sesoi_sweep_faceted, aes(x = SESOI, y = prop_outside_SESOI)) +
      geom_line() +
      geom_point(size = 1) +
      scale_y_continuous(breaks = c(0.0, 0.25, 0.50, 0.75, 1.0),
                         labels = c("0.0", "0.25", "0.50", "0.75", "1.0"),
                         limits = c(0, 1)
                         ) +
      scale_x_continuous(breaks = c(0.0, 0.25, 0.50, 0.75, 1.0),
                         labels = c("0.0", "0.25", "0.50", "0.75", "1.0"),
                         limits = c(0, 1)
                         ) +
      labs(
        title = "Proportion bootstraps outside SESOI",
        x = "SESOI",
        y = "Proportion outside SESOI [0-1]"
      ) +
      theme_bw() +
  facet_wrap(~facet_label,
             ncol = 5)

# Save multiplot
ggsave(
      filename = "SESOI_multiplot.png",
      plot = sesoi_multiplot,
      path = here::here("equivalence_test_analysis", "outputs", "sesoi"),
      width = 10,
      height = 8,
      dpi = 600,
      bg = "white"
    )
```



## Different visualisation: stacked bar plot of "equivalence classification results" based on SESOI choice
In other words, visualise of choice of SESOI affects results for each metric pairwise-comparisons.

### First: Calculation across metrics 
```{r}
# Turn equivalence col into factor
sesoi_sweep_all$equivalence <- as.factor(sesoi_sweep_all$equivalence)

# Calculate number of pairwise comparisons yielding to unique equivalence result
sesoi_sweep_all_proportions <- sesoi_sweep_all %>%
  group_by(SESOI, equivalence) %>%
  summarise(count = n(), .groups = "drop") 


sesoi_barplot <- 
  ggplot(sesoi_sweep_all_proportions, aes(x = as.factor(SESOI),
                                        fill = equivalence,
                                        weight = count)) +
  geom_bar(position = "fill",
           alpha = 0.9) +
  labs(
    title = "Equivalence test results across pairwise-comparisons based on SESOI",
    x = "SESOI",
    y = "Proportion",
    fill = "Equivalence test outcome"
  ) +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal()

# Save barplot
ggsave(
      filename = "eq_results_barplot.png",
      plot = sesoi_barplot,
      path = here::here("equivalence_test_analysis", "outputs"),
      width = 8,
      height = 6,
      dpi = 600,
      bg = "white"
    )

```

## Creating gt tables to report statistical analyses

### First: Table for Friedman test
```{r}
friedman_table <- friedman_all %>%
  select(-c(.y., method)) %>% # do not need this col 
  # re-order columns
  dplyr::relocate(metric, .before= n) %>%
  dplyr::relocate(p.signif, .before= kendallsw) %>% 
  dplyr::relocate(df, .before = statistic) %>% 
  # round decimal numbers
  mutate(p = round(p, 5),
         statistic = round(statistic, 2),
         kendallsw = round(kendallsw, 2)
         ) %>%
  # Rename values in rows to make output prettier
  mutate(
    metric = case_when(
      metric == "mean" ~ "Average daytime mEDI",
      metric == "m10" ~ "M10m",
      metric == "iv" ~ "IV",
      metric == "is" ~ "IS",
      TRUE ~ metric
      )
    ) %>%
  gt::gt() %>%
  gt::tab_header(title =gt::md("**Friedman test results**")) %>% # title for the whole table
  # Workaround to create superscipts in the table
  gt::text_transform(
  locations = gt::cells_body(columns = c(metric)),
  fn = function(x) {
    sapply(x, function(val) {
      if (str_detect(val, "^(tat|llit|mlit|flit)\\d+")) {
        prefix <- str_extract(val, "^[a-z]+")
        sub <- str_extract(val, "\\d+")

        formatted <- case_when(
          prefix == "tat" ~ glue::glue("TAT<sub>{sub}</sub>"),
          prefix == "llit" ~ glue::glue("LLiT<sub>{sub}</sub>"),
          prefix == "mlit" ~ glue::glue("MLiT<sub>{sub}</sub>"),
          prefix == "flit" ~ glue::glue("FLiT<sub>{sub}</sub>"),
          TRUE ~ val
        )

        gt::html(formatted)
      } else {
        gt::html(val)
      }
    })
  }
) %>%
  gt::cols_label(metric = "Metric",
                 n = "N",
                 df = "Df",
                 statistic = "Statistic",
                 kendallsw = "Kendall's W",
                 p.signif = "Significance")

print(friedman_table)

gt::gtsave(friedman_table,
           filename = "friedman_table.png",
           path = here::here("outputs"),
           vwidth = 1450,
           vheight = 800)

  
```

### Second: Table for post-hoc comparisons and equivalence testing
```{r}
posthoc_equiv_table <- equiv_results %>%
  select(-c(.y., prop_inside_SESOI, prop_outside_SESOI, plot_obj, SESOI)) %>% #we do not need these cols in the final summary
  # again, re-order columns
  dplyr::relocate(metric, .before= group1) %>%
  mutate(p = round( p, 4),
         p.adj = round(p.adj, 4),
         cohens_d= round(cohens_d, 4),
         lower_CI = round(lower_CI, 4),
         upper_CI = round(upper_CI, 4)
  ) %>%
  # Rename values in rows to make output prettier
  ## col metric
  mutate(
    metric = case_when(
      metric == "mean" ~ "Average daytime mEDI",
      metric == "m10" ~ "M10m",
      metric == "iv" ~ "IV",
      TRUE ~ metric
      )
    ) %>%
  ## cols group 1 and group 2
  mutate(
    group1 = case_when(
      group1 == "raw" ~ "Raw data",
      group1 == "wrlg" ~ "Wear log-corrected",
      group1 == "clusters" ~ "Algorithm-corrected"
      )
    ) %>%
  mutate(
    group2 = case_when(
      group2 == "raw" ~ "Raw data",
      group2 == "wrlg" ~ "Wear log-corrected",
      group2 == "clusters" ~ "Algorithm-corrected"
      )
    ) %>%
  ## col equivalence
  mutate(
    equivalence = case_when(
      equivalence == "positive effect" ~ "Positive effect",
      equivalence == "negative effect" ~ "Negative effect",
      equivalence == "inconclusive" ~ "Inconclusive")
    ) %>%
  # Making new col for directionality
  mutate(direction = case_when(
    equivalence == "Positive effect" ~ paste0(group1, " > ", group2),
    equivalence == "Negative effect" ~ paste0(group1, " < ", group2),
    equivalence == "Inconclusive" ~ "/")
    ) %>%
  # # Making the rows look nicer for later gt table output
  # ## col direction
  # mutate(
  #   direction = case_when(
  #     direction == "Raw data > Wear log-corrected" ~ "Raw > Wear log-corrected",
  #     direction == "Raw data < Wear log-corrected" ~ "Raw < Wear log-corrected",
  #     direction == "Raw data > Algorithm-corrected" ~ "Raw > Algorithm-corrected",
  #     direction == "Raw data < Algorithm-corrected" ~ "Raw < Algorithm-corrected",
  #     direction == "Wear log-corrected < Algorithm-corrected" ~ "Wear log < Algorithm-corrected")
  #   ) %>%
  gt::gt() %>% 
  gt::tab_header(title = gt::md("**Post-hoc Wilcoxon pairwise comparisons and equivalence test results**"),
                 subtitle = gt::md("**Smallest effect size of interest for equivalence testing = 0.3**")
                 ) %>% #Add title
  # Workaround to get superscript formatting using HTML within the rows
  gt::text_transform(
    locations = gt::cells_body(
      columns = c(metric)
    ),
    fn = function(x) {
    sapply(x, function(val) {
      if (str_detect(val, "^tat\\d+")) {
        sub <- str_extract(val, "\\d+")
        gt::html(glue::glue("TAT<sub>{sub}</sub>"))
      } else if (str_detect(val, "^llit\\d+")) {
        sub <- str_extract(val, "\\d+")
        gt::html(glue::glue("LLiT<sub>{sub}</sub>"))
      } else {
        gt::html(val)
      }
    })
  }
) %>%
  gt::tab_style(
  style = gt::cell_borders(
    sides = "right",
    color = "darkgrey"
  ),
  locations = gt::cells_body(columns = "metric")
) %>%
  gt::cols_label(metric = "Metric",
             group1 = "Group 1",
             group2 = "Group 2",
             n1 = "N1",
             n2 = "N2",
             statistic = "Statistic",
             cohens_d = "Cohen's d",
             p.adj = "Adjusted p",
             p.adj.signif = "Significance",
             lower_CI = "Lower CI",
             upper_CI = "Upper CI",
             equivalence = "Equivalence test result",
             direction = "Directionality of effect") 

posthoc_equiv_table

gt::gtsave(posthoc_equiv_table,
           filename = "posthoc_equiv_table.png",
           path = here::here("outputs"),
           vwidth = 1950,
           vheight = 800,
           # To make sure table does not get cropped out
           expand = c(5, #top
                      20, #right
                      5, #bottom
                      15)) #left

  
```

## Third, option 1: Table for meaningfully positive or negative effects of the equivalence test
```{r}
equiv_meanginful_results <- equiv_results %>%
  filter(equivalence == "positive effect" | equivalence == "negative effect") %>%
  # Creating a column for directionality of effect
  mutate(direction = if_else(equivalence == "positive effect", paste0(group1, " > ", group2),
                             paste0(group1, " < ", group2)
                             ),
         lower_CI = round(lower_CI, 2),
         upper_CI = round(upper_CI, 2)
         ) %>%
  # Making the rows look nicer for later gt table output
  ## col direction
  mutate(
    direction = case_when(
      direction == "raw > wrlg" ~ "Raw > Wear log",
      direction == "raw < wrlg" ~ "Raw < Wear log",
      direction == "raw > clusters" ~ "Raw > Algorithm",
      direction == "raw < clusters" ~ "Raw < Algorithm",
      direction == "wrlg < clusters" ~ "Wear log < Algorithm")
    ) %>%
  # Change other rows
  ## col metric
  mutate(
    metric = case_when(
      metric == "mean" ~ "Average daytime mEDI",
      metric == "m10" ~ "M10m",
      TRUE ~ metric
      )
    ) %>%
  ## cols group 1 and group 2
  mutate(
    group1 = case_when(
      group1 == "raw" ~ "Raw",
      group1 == "wrlg" ~ "Wear log",
      group1 == "clusters" ~ "Algorithm"
      )
    ) %>%
  mutate(
    group2 = case_when(
      group2 == "raw" ~ "Raw",
      group2 == "wrlg" ~ "Wear log",
      group2 == "clusters" ~ "Algorithm"
      )
    ) %>%
  ## col equivalence
  mutate(
    equivalence = if_else(equivalence == "positive effect", "Positive effect", "Negative effect")
  ) %>%
  dplyr::select(-c(.y., n1, n2, statistic, p, p.adj, p.adj.signif, cohens_d, prop_inside_SESOI, prop_outside_SESOI, SESOI, plot_obj)) %>%
  dplyr::relocate(metric, .before= group1) %>%
  gt::gt() %>%
  gt::tab_header(title = gt::md("**Equivalence testing results: positive and negative effects**"),
                 subtitle = gt::md("**Smallest effect size of interest = 0.3**")
                 ) %>%
  # Workaround to get superscript formatting using HTML within the rows
  gt::text_transform(
    locations = gt::cells_body(
      columns = c(metric)
    ),
    fn = function(x) {
      sapply(x, function(val) {
        if (str_detect(val, "tat\\d+")) {
          sup <- str_extract(val, "\\d+")
          gt::html(glue::glue("TAT<sup>{sup}</sup>"))
        } else {
          gt::html(val)
        }
      })
    }
  ) %>%

  gt::cols_label(metric = "Metric",
                 equivalence = "Equivalence test result",
                 group1 = "Group 1",
                 group2 = "Group 2",
                 lower_CI = "Lower CI",
                 upper_CI = "Upper CI",
                 direction = "Directionality of effect"
                 )

# Saving table
gt::gtsave(equiv_meanginful_results,
           filename = "equiv_meanginful_results.png",
           path = here::here("outputs"),
           vwidth = 1000,
           vheight = 800)
  
```
## Third, option 2: show same thing in a 2x2 table 
We need to create this table manually 
```{r}
# Create data with a grouping label
equiv_tibble_manual <- tibble::tibble(
  Group = c("Difference between correction methods", "Difference between correction methods"),
  Difference = c("Inconclusive", "Yes"),
  `Inconclusive` = c(
    "LLiT<sub>10</sub><br>LLiT<sub>1000</sub><br>IV<br>IS",
    "/"
  ),
  `Yes` = c(
    "<span style='color:red;'>Average daytime mEDI<br>M10m</span>",
    "<span style='color:green;'>TAT<sub>1000</sub><br>TAT<sub>250</sub></span>"
  )
)


# Build gt table with group_rows to simulate vertical merge
equiv_table_manual <- equiv_tibble_manual %>%
  gt::gt(groupname_col = "Group") %>%
  tab_spanner(
    label = md("**Difference from no correction**"),
    columns = c(Inconclusive, Yes)
  ) %>%
  # Insert separation line after Difference col
  gt::tab_style(
  style = gt::cell_borders(
    sides = "right",
    color = "lightgrey",
    weight = px(2)
  ),
  locations = gt::cells_body(columns = "Difference")
) %>%
  # Insert separation line after Group col 
  gt::tab_style(
  style = gt::cell_borders(
    sides = "right",
    color = "lightgrey"
  ),
  locations = gt::cells_body(columns = "Group")
  ) %>%
  # Make text bold in the Difference col
    tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_body(
      columns = "Difference",
      rows = Difference %in% c("Yes", "Inconclusive")
    )
  ) %>%
  # Hide the Difference col name, as we do not need it
  cols_label(
    Difference = md("")
  ) %>%
  fmt_markdown(columns = everything()) %>%
  # Format all cols to be bold
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels(everything())
  ) %>%
  tab_style(
    style = cell_text(weight = "bold", align = "center"),
    locations = cells_row_groups(groups = "Difference between correction methods")
  ) %>%
  # Centre the col names 
  tab_style(
    style = cell_text(align = "center", weight = "bold"),
    locations = cells_column_labels(columns = c("Inconclusive", "Yes"))
    ) %>%
  tab_options(
    table.width = pct(90),
    row_group.as_column = TRUE
  ) %>%
  # Left border on the first column (Group)
  tab_style(
    style = cell_borders(
      sides = "left",
      color = "lightgrey",
      weight = px(2)
    ),
    locations = cells_row_groups()
  ) %>%
  # Right border within table
  tab_style(
  style = cell_borders(
    sides = "right",
    color = "lightgrey",
    weight = px(1)
  ),
  locations = cells_body(columns = "Inconclusive")
) %>%
  # Right border on the last column (Yes)
  tab_style(
    style = cell_borders(
      sides = "right",
      color = "lightgrey",
      weight = px(2)
    ),
    locations = cells_body(columns = "Yes")
  ) %>%
  # Line between Inconclusive and yes columns
  tab_style(
  style = cell_borders(
    sides = c("right"),
    color = "lightgrey",
    weight = px(1)
  ),
  locations = cells_column_labels(columns = "Inconclusive")
) %>%
  # Line after the yes col, to close the table
  tab_style(
    style = cell_borders(
    sides = c("right"),
    color = "lightgrey",
    weight = px(2)
  ),
  locations = cells_column_labels(columns = "Yes")
) %>%
  # Line before the Incoclusive col, to close the table
  tab_style(
    style = cell_borders(
    sides = c("left"),
    color = "lightgrey",
    weight = px(2)
  ),
  locations = cells_column_labels(columns = "Inconclusive")
) %>%  
  tab_options(
  column_labels.border.top.width = px(0),
  column_labels.border.top.color = "red"
) %>%
  tab_source_note(
    md("<span style='color:red;'>Uncorrected metric value is lower</span><br>
        <span style='color:green;'>Uncorrected metric value is higher</span>")
  )

equiv_table_manual

# Saving table
gt::gtsave(equiv_table_manual,
           filename = "equiv_table_manual.png",
           path = here::here("outputs"),
           vwidth = 800,
           vheight = 600)


```



## Calculating mean and sd of all timing functions in order to report result of the t-test
```{r}
# Define a function that calculates the mean of each id metric
## Note that this function uses the function style_time, which is already imported in 06_metrics_comparison.Rmd from the file vis_metrics_funs.R

metrics_means <- function(df, df_name) {
  
  # Compute means and sds
  df <- df %>%
    summarise(
      mean_ids_raw = style_time(mean(as.numeric(mean_raw), na.rm = TRUE)),
      sd_ids_raw = style_time(sd(as.numeric(mean_raw), na.rm = TRUE)),
      mean_ids_wrlg = style_time(mean(as.numeric(mean_wrlg), na.rm = TRUE)),
      sd_ids_wrlg = style_time(sd(as.numeric(mean_wrlg), na.rm = TRUE)),
      mean_ids_clusters = style_time(mean(as.numeric(mean_clusters), na.rm = TRUE)),
      sd_ids_clusters = style_time(sd(as.numeric(mean_clusters), na.rm = TRUE))
    )
  
  #Adding col to specify which metric and rename the df
  df$metric <- df_name
  
  return(df)
}

# Now we run a for loop on different metric dfs to calculate the mean 

# Create a named list of data frames
metrics_dfs <- list(
  mlit250_all = mlit250_all,
  mlit1000_all = mlit1000_all,
  llit10_all = llit10_all,
  llit250_all = llit250_all,
  llit1000_all = llit1000_all,
  flit10_all = flit10_all,
  flit250_all = flit250_all,
  flit1000_all = flit1000_all,
  tat1000_all = tat1000_all,
  tat250_all = tat250_all
)

# Initiate empty list to store results
means_metric_list <- list()

# Create for loop 
for (df_name in names(metrics_dfs)) {
 
   df <- metrics_dfs[[df_name]]
  
  # Compute the means for the current data frame
  df_means <- metrics_means(df, df_name)
  
  # Store the resulting data frame in the list, using the index i
  means_metric_list[[df_name]] <- df_means
}

# Convert the list of data frames to a single data frame
means_metrics_df <- bind_rows(means_metric_list)
```

## Calculating mean and sd of is and iv and the "quantity metrics" (mean and M10)

```{r}
is_means <- is_all %>%
    summarise(
      mean_ids_raw = mean(IS_raw, na.rm = TRUE),
      sd_ids_raw = sd(IS_raw, na.rm = TRUE),
      mean_ids_wrlg = mean(IS_wrlg, na.rm = TRUE),
      sd_ids_wrlg = sd(IS_wrlg, na.rm = TRUE),
      mean_ids_clusters = mean(IS_clusters, na.rm = TRUE),
      sd_ids_clusters = sd(IS_clusters, na.rm = TRUE),
    ) %>%
  mutate(metric = "is")

iv_means <- iv_all %>%
  summarise(
      mean_ids_raw = mean(IV_raw, na.rm = TRUE),
      sd_ids_raw = sd(IV_raw, na.rm = TRUE),
      mean_ids_wrlg = mean(IV_wrlg, na.rm = TRUE),
      sd_ids_wrlg = sd(IV_wrlg, na.rm = TRUE),
      mean_ids_clusters = mean(IV_clusters, na.rm = TRUE),
      sd_ids_clusters = sd(IV_clusters, na.rm = TRUE),
    ) %>%
  mutate(metric = "iv")

mean_means <- mean_all %>%
  summarise(
      mean_ids_raw = mean(mean_raw, na.rm = TRUE),
      sd_ids_raw = sd(mean_raw, na.rm = TRUE),
      mean_ids_wrlg = mean(mean_wrlg, na.rm = TRUE),
      sd_ids_wrlg = sd(mean_wrlg, na.rm = TRUE),
      mean_ids_clusters = mean(mean_clusters, na.rm = TRUE),
      sd_ids_clusters = sd(mean_clusters, na.rm = TRUE),
    ) %>%
  mutate(metric = "mean (daytime)")

m10_means <- m10_all %>%
  summarise(
      mean_ids_raw = mean(mean_raw, na.rm = TRUE),
      sd_ids_raw = sd(mean_raw, na.rm = TRUE),
      mean_ids_wrlg = mean(mean_wrlg, na.rm = TRUE),
      sd_ids_wrlg = sd(mean_wrlg, na.rm = TRUE),
      mean_ids_clusters = mean(mean_clusters, na.rm = TRUE),
      sd_ids_clusters = sd(mean_clusters, na.rm = TRUE),
    ) %>%
  mutate(metric = "m10")


```

## Now we would like to have a single table for all metrics.

The issue with this is that bind_rows() will not merge cols that are of different data types (hms and numeric, in our case). Hence, we first convert everything to character, and then combine. We can do this since we are creating this table for display only, and not for any calculations.

```{r}
# Convert means_metrics_df cols to character
means_metrics_df <- means_metrics_df %>%
  mutate(dplyr::across(.fns = as.character))

# Convert numeric cols to character and round to 4 digits
iv_means <- iv_means %>%
  mutate(dplyr::across(where(is.numeric), ~ format(.x, digits = 4)))
is_means <- is_means %>%
  mutate(dplyr::across(where(is.numeric), ~ format(.x, digits = 4)))
mean_means <- mean_means %>%
  mutate(dplyr::across(where(is.numeric), ~ format(.x, digits = 4)))
m10_means <- m10_means %>%
  mutate(dplyr::across(where(is.numeric), ~ format(.x, digits = 4)))
  
# Combine the data using bind_rows
means_dfs <- dplyr::bind_rows(means_metrics_df, is_means, iv_means, mean_means, m10_means)

# Rename some columns to eliminate the "_all" from the metric col
means_dfs <- means_dfs %>%
  mutate(metric = stringr::str_replace(metric, "_all$", "")) %>%
  dplyr::relocate(metric, .before= mean_ids_raw) # re-order columns so that metric is first one
```

### Create gt table for this dataframe

```{r}
means_table <- means_dfs %>%
  # Rename values in rows to make output prettier
  mutate(
    metric = case_when(
      metric == "mean" ~ "Average daytime mEDI",
      metric == "m10" ~ "M10m",
      metric == "iv" ~ "IV",
      metric == "is" ~ "IS",
      TRUE ~ metric
      )
    ) %>%
  gt::gt() %>%
  gt::tab_header(title = gt::md("**Means and SDs of light exposure metrics (n=14)**")) %>% #Add title
  # Workaround to create superscripts
  gt::text_transform(
  locations = gt::cells_body(columns = c(metric)),
  fn = function(x) {
    sapply(x, function(val) {
      if (str_detect(val, "^(tat|llit|mlit|flit)\\d+")) {
        prefix <- str_extract(val, "^[a-z]+")
        sup <- str_extract(val, "\\d+")

        formatted <- case_when(
          prefix == "tat" ~ glue::glue("TAT<sup>{sup}</sup>"),
          prefix == "llit" ~ glue::glue("LLiT<sup>{sup}</sup>"),
          prefix == "mlit" ~ glue::glue("MLiT<sup>{sup}</sup>"),
          prefix == "flit" ~ glue::glue("FLiT<sup>{sup}</sup>"),
          TRUE ~ val
        )

        gt::html(formatted)
      } else {
        gt::html(val)
      }
    })
  }
) %>%
  gt::cols_label(metric = "Metric",
             mean_ids_raw = "Raw dataset (mean)",
             sd_ids_raw = "Raw dataset (SD)",
             mean_ids_wrlg = "Clean (Wear log) dataset (mean)",
             sd_ids_wrlg = "Clean (Wear log) dataset (SD)",
             mean_ids_clusters = "Clean (algorithm) dataset (mean)",
             sd_ids_clusters = "Clean (algorithm) dataset (SD)") %>%
  gt::cols_align(align = "center",
                 columns = dplyr::everything()) 

# Save the gt table
## Make sure you set your path to Chrome using Sys.setenv(CHROMOTE_CHROME = "path/to/chrome.exe") and check this was correct by running chromote::find_chrome()


gt::gtsave(means_table,
           filename = "means_table.png",
           path = here::here("outputs"))




```

