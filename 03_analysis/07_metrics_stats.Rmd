---
title: "metrics_stats"
author: "Carolina Guidolin"
date: "2024-12-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Aim

To calculate differences between metrics calculated in the script 06_metrics_comparison.Rmd. Specifically, we will be using a t-test to do this.

## Let's first check the normality of our data

### Import the norm_check functions before running this as well as necessary packages

```{r}
# Function to check normality
base::source(here::here("03_analysis", "functions", "norm_check_funs.R"))

library(ggpubr)
library(rstatix)
library(tidyverse)
```
### Importing metrics dfs if they are not already in global environment
```{r}
metric_files <- list.files(
  path = here::here("outputs"), # choose directory based on where data was saved
  pattern = "\\.csv$",
  full.names = TRUE
)

# Loop through each file and assign to the global environment
for (file in metric_files) {
  df_name <- tools::file_path_sans_ext(basename(file))
  assign(
    df_name,
    readr::read_csv(file, show_col_types = FALSE),
    envir = .GlobalEnv
  )
}
```

### Testing for normality of residuals

```{r}
options(scipen = 999) # avoid scientific notation 
set.seed(20250709)

# Create list of data frames containing metric values
light_metrics_list <- list(
  mlit250_all = mlit250_all,
  mlit1000_all = mlit1000_all,
  llit10_all = llit10_all,
  llit250_all = llit250_all,
  llit1000_all = llit1000_all,
  flit10_all = flit10_all,
  flit250_all = flit250_all,
  flit1000_all = flit1000_all,
  tat1000_all = tat1000_all,
  tat250_all = tat250_all,
  mean_all = mean_all,
  m10_all = m10_all,
  iv_all = iv_all,
  is_all = is_all
)

# Visualise normality of the means for each df
for (i in seq_along(light_metrics_list)) {
  df <- light_metrics_list[[i]]
  df_name <- names(light_metrics_list)[i]
  print(norm_check_means(df, df_name = df_name))
}


## The distribution does not look normal, and for several metrics the Shapiro Wilk test also returns significant results
## Hence, non-parametric alternative is needed

```

### Performing the non-parametric Friedman test for the metrics, and calculate pairwise comparisons (post-hoc tests) using Wilcoxon test

The first step is some renaming of columns across datasets, i.e. changing cols to "raw", "clusters", "wrlg" and eliminating the "_all" from the metric
```{r}
# Keep copy (need for later)
light_metrics_cleaned <- lapply(light_metrics_list, function(df) {
  # Make a copy of the dataframe
  df_new <- df

  # Rename mean, IV, and IS columns
  names(df_new) <- sub("^mean_raw$", "raw", names(df_new))
  names(df_new) <- sub("^mean_wrlg$", "wrlg", names(df_new))
  names(df_new) <- sub("^mean_clusters$", "clusters", names(df_new))

  names(df_new) <- sub("^IV_raw$", "raw", names(df_new))
  names(df_new) <- sub("^IV_wrlg$", "wrlg", names(df_new))
  names(df_new) <- sub("^IV_clusters$", "clusters", names(df_new))
  
  names(df_new) <- sub("^IS_raw$", "raw", names(df_new))
  names(df_new) <- sub("^IS_wrlg$", "wrlg", names(df_new))
  names(df_new) <- sub("^IS_clusters$", "clusters", names(df_new))

  # Remove 'delta_' and 'sd_' columns
  df_new <- df_new[ , !grepl("^delta_|^sd_", names(df_new)) ]

  return(df_new)
})
```


#### Function to perform the two test for all metrics

```{r}
# Friedman test
## First, we create a function for the metrics that have been calculated as average of 6 participation days
## These are: mlit250, mlit1000, llit10, llit250, llit1000, flit10, flit250, flit1000, tat1000, tat250, mean (daytime, M10m)

friedman_and_posthoc <- function(df, pval_threshold = 0.05, metric_name = NULL) {
  
  # Get the name of the input df as a string
    if (!is.null(metric_name)) {
      metric_name <- sub("_all$", "", metric_name) # removes _all from metric name
      } else {
        metric_name <- "unknown_metric"
        }
  
  # Turn into long format
  df_long <- df %>%
    pivot_longer(
      cols = c("raw", "wrlg", "clusters"),
      names_to = "dataset",
      values_to = "value"
    ) %>%
    mutate(value = as.numeric(value),
           Id = as.factor(Id),
           dataset = factor(dataset, levels = c("raw", "wrlg", "clusters")) # order the levels, makes the post-hoc comparisons easier to interpret
           ) %>%
    ungroup()
  
  summary(df_long$value)
  
  # Friedman test and effect size (Kendall's W)
  friedman <- df_long %>%
    rstatix::friedman_test(value ~ dataset | Id) %>%
    mutate(
      kendallsw = rstatix::friedman_effsize(df_long, value ~ dataset | Id)$effsize,
      metric = metric_name
    )
  
  # Post-hoc only if friedman test is significant
  if (friedman$p < pval_threshold) {
    # Perform Wilcoxon pairwise comparisons
    posthoc <- df_long %>%
      rstatix::pairwise_wilcox_test(
        value ~ dataset,
        paired = TRUE,
        p.adjust.method = "fdr"
      ) %>%
      # Add metric name
      mutate(
        metric = metric_name # add metric name
    )
  } 
  else {
  posthoc <- NULL
  }
  
return(list(
    friedman = friedman,
    posthoc = posthoc
  ))
  
}

```

### Apply Friedman testing function to all metrics dfs

```{r}
library(purrr)

# Use list of light metrics indicated above

results <- lapply(names(light_metrics_cleaned), function(nm) {
  fried_res <- friedman_and_posthoc(light_metrics_cleaned[[nm]], 0.05, nm)
  return(fried_res)
})

# Save results in dfs
friedman_all <- purrr::map_dfr(results, "friedman")
posthoc_all  <- purrr::map_dfr(results, "posthoc") 

# Rename some 
```

### Performing equivalence testing

The main idea of performing equivalence testing here is to compare the metrics across datasets and identify whether the difference is small enough to treat the three datasets as practically the same. We perform equivalence testing using bootstrapping. Bootstrapping is a way to get an empirical distribution of possible effect sizes (as if we repeated the study many times), i.e. estimating the sampling distribution of our effect size. So put simply, bootstrapping gives us a way to answer: How consistent (robust) is my evidence that this difference is small enough to be ignorable?

### Applying bootstrap eq testing

```{r}

# First, rename the metric names in light_metric_list so that do not contain _all after the metric name
names(light_metrics_cleaned) <- sub("_all$", "", names(light_metrics_cleaned))

run_bootstrap_equiv_test <- function(metric_name, group1, group2, light_metrics_cleaned,
                                     n_bootstraps = 10^4, SESOI = NULL, alpha = 0.05) {
  
  # Pull the original data
  df <- light_metrics_cleaned[[metric_name]]
  if (is.null(df)) stop(paste0("Metric '", metric_name, "' not found!"))
  
  data1 <- as.numeric(df[[group1]])
  data2 <- as.numeric(df[[group2]])
  
  # Checking if cols are correctly identified 
  if (any(is.null(c(data1, data2)))) {
    return(tibble::tibble(
      error = paste0("Group columns not found: ", group1, ", ", group2),
      cohens_d = NA_real_,
      lower_CI = NA_real_,
      upper_CI = NA_real_,
      is_equivalent = NA,
      prop_inside_SESOI = NA_real_,
      prop_outside_SESOI = NA_real_
    ))
  }
  
  # Calculate effect size and bootstraps
  effect.size <- effectsize::cohens_d(data1, data2, paired = TRUE)$Cohens_d
  #Apply costum function
  bootstraps <- bootstrap_differences(n_bootstraps, data1, data2)
  
  # Filtering non-NA bootstraps 
  bootstraps <- bootstraps[!is.na(bootstraps)]
  
  if (length(bootstraps) == 0) {
    return(tibble::tibble(
      error = "All bootstrap samples are NA",
      cohens_d = effect.size,
      lower_CI = NA_real_,
      upper_CI = NA_real_,
      is_equivalent = NA,
      prop_inside_SESOI = NA_real_,
      prop_outside_SESOI = NA_real_
    ))
  }
  
  # Calculate proportion inside and outside of SESOI
  prop_inside_SESOI <- sum(bootstraps >= -SESOI & bootstraps <= SESOI, na.rm = TRUE)/
                  length(bootstraps)
  prop_outside_SESOI <- sum(bootstraps < -SESOI | bootstraps > SESOI, na.rm = TRUE)/
                  length(bootstraps)

  # Visualise the bootstraps
  ci <- quantile(bootstraps, probs = c(0.05, 0.95))
  lower_CI <- ci[1]
  upper_CI <- ci[2]
  
  plot_obj <- tibble::tibble(value = bootstraps) |>
  ggplot(aes(x = value)) +
  geom_histogram(binwidth = 0.02,
                 aes(fill = value >= SESOI | value <= -SESOI)) +
  coord_cartesian(xlim = c(-1.5, 1.5)) +
  labs(
    title = paste0("Bootstrap distribution for ", metric_name, ": ", group1, " vs ", group2),
    subtitle = paste0("Smallest effect size of interest (SESOI):", SESOI),
    x = "Effect size",
    fill = "Effect larger than SESOI"
  ) +
  # Add SESOI region, CIs and observed eff size 
  geom_vline(
    aes(xintercept = effect.size, color = "Observed Cohen's d"),
  linetype = "dashed",
  linewidth = 1
  ) +
  geom_vline(
    aes(xintercept = lower_CI, color = "CI bounds"),
  linetype = "dashed",
  linewidth = 1
  ) +
  geom_vline(
    aes(xintercept = upper_CI, color = "CI bounds"),
    linetype = "dashed",
    linewidth = 1
  ) +
  # Shaded SESOI region
  annotate(
    "rect",
    xmin = -SESOI, xmax = SESOI,
    ymin = 0, ymax = Inf,
    fill = "grey", alpha = 0.2
  ) +
  scale_fill_manual(
    values = c(
      "FALSE" = "lightblue",   
      "TRUE" = "orange"        
    )
  ) +
  scale_color_manual(
    values = c(
      "Observed Cohen's d" = "red",
      "CI bounds" = "blue"
    )
  ) +
  theme_minimal()

  print(plot_obj)
  
    # Save plot with dynamic name
  ggsave(
  paste0(
    "bootstrap_",
    metric_name, "_",
    group1, "_vs_", group2, ".png"
  ),
  plot = plot_obj,
  path = here::here("equivalence_test_analysis", "outputs"),
  width = 8,
  height = 5,
  bg = "white",
  dpi = 600
)

  
  # Calculating confidence intervals and classifying results 
  ci <- quantile(bootstraps, probs = c(alpha, 1 - alpha), na.rm = TRUE)
  eq_decision <- if ((ci[1] >= -SESOI) && (ci[2] <= SESOI)) {
    "equivalence"
  } else if ((ci[1] < -SESOI) && (ci[2] < -SESOI)) {
    "meanginfully negative effect"
  } else if ((ci[1] > SESOI) && (ci[2] > SESOI)) {
    "meaningfully positive effect"
  } else {
    "inconclusive"
  }
    
  
  tibble::tibble(
    cohens_d = effect.size,
    lower_CI = ci[1],
    upper_CI = ci[2],
    equivalence = eq_decision,
    prop_inside_SESOI = prop_inside_SESOI,
    prop_outside_SESOI = prop_outside_SESOI,
    SESOI = SESOI
  )
}

```


## Try out on all pairwise combinations from the post-hoc tests
Interpretation of the results
- If both ends of the CI lie within SESOI bounds: equivalence
- If any part of the CI extends outside SESOI, can’t conclude equivalence -> inconclusive
- If both CIs are below SESOI: meaningfully negative effect
- Both CIs are above SESOI: meaningfully positive effect

```{r}
# Import boottrapping function
base::source(here::here("03_analysis", "functions", "bootstrapping_fun.R"))
```

```{r}
# Adding equivalence to post-hoc results 
equiv_results <- posthoc_all %>%
  mutate(equiv_result = pmap(
    list(metric, group1, group2),
    # For each row, run bootstrap eq test on pairs of groups for given metric using original data in metrics_list 
    ~ run_bootstrap_equiv_test(..1, ..2, ..3, SESOI = 0.3, light_metrics_cleaned)
  )) %>%
  tidyr::unnest(equiv_result)

```
### Visualising how the proportion inside/outside the SESOI varies based on SESOI values

```{r}
# Same function but without the plot being created, and saving the sesoi
noplot_bootstrap_equiv_test <- function(metric_name, group1, group2, light_metrics_cleaned,
                                     n_bootstraps = 10^4, SESOI = NULL, alpha = 0.05) {
  
  # Pull the original data
  df <- light_metrics_cleaned[[metric_name]]
  if (is.null(df)) stop(paste0("Metric '", metric_name, "' not found!"))
  
  data1 <- as.numeric(df[[group1]])
  data2 <- as.numeric(df[[group2]])
  
  # Check columns
  if (any(is.null(c(data1, data2)))) {
    return(tibble::tibble(
      error = paste0("Group columns not found: ", group1, ", ", group2),
      cohens_d = NA_real_,
      lower_CI = NA_real_,
      upper_CI = NA_real_,
      is_equivalent = NA,
      prop_inside_SESOI = NA_real_,
      prop_outside_SESOI = NA_real_,
      SESOI = SESOI
    ))
  }
  
  # Calculate effect size and bootstraps
  effect.size <- effectsize::cohens_d(data1, data2, paired = TRUE)$Cohens_d
  bootstraps <- bootstrap_differences(n_bootstraps, data1, data2)
  bootstraps <- bootstraps[!is.na(bootstraps)]
  
  if (length(bootstraps) == 0) {
    return(tibble::tibble(
      error = "All bootstrap samples are NA",
      cohens_d = effect.size,
      lower_CI = NA_real_,
      upper_CI = NA_real_,
      is_equivalent = NA,
      prop_inside_SESOI = NA_real_,
      prop_outside_SESOI = NA_real_,
      SESOI = SESOI
    ))
  }
  
  # Calculate proportion inside and outside of SESOI
  prop_inside_SESOI <- sum(bootstraps >= -SESOI & bootstraps <= SESOI, na.rm = TRUE)/
                  length(bootstraps)
  prop_outside_SESOI <- sum(bootstraps < -SESOI | bootstraps > SESOI, na.rm = TRUE)/
                  length(bootstraps)
  
  # Calculating confidence intervals and classifying results 
  ci <- quantile(bootstraps, probs = c(alpha, 1 - alpha), na.rm = TRUE)
  eq_decision <- if ((ci[1] >= -SESOI) && (ci[2] <= SESOI)) {
    "equivalence"
  } else if ((ci[1] < -SESOI) && (ci[2] < -SESOI)) {
    "meanginfully negative effect"
  } else if ((ci[1] > SESOI) && (ci[2] > SESOI)) {
    "meaningfully positive effect"
  } else {
    "inconclusive"
  }
  
  tibble::tibble(
    cohens_d = effect.size,
    lower_CI = ci[1],
    upper_CI = ci[2],
    is_equivalent = eq_decision,
    prop_inside_SESOI = prop_inside_SESOI,
    prop_outside_SESOI = prop_outside_SESOI,
    SESOI = SESOI
  )
}

sesoi_seq <- seq(0, 1, by = 0.1)

# Empty tibble to store results
sesoi_sweep_all <- tibble(
  metric = character(),
  group1 = character(),
  group2 = character(),
  SESOI = numeric(),
  prop_outside_SESOI = numeric(),
  equivalence = character()
)

# Loop over posthoc_all rows and SESOI values
for (i in seq_len(nrow(posthoc_all))) {
  
  metric_name <- posthoc_all$metric[i]
  group1 <- posthoc_all$group1[i]
  group2 <- posthoc_all$group2[i]
  
  for (s in sesoi_seq) {
    
    res <- noplot_bootstrap_equiv_test(
      metric_name = metric_name,
      group1 = group1,
      group2 = group2,
      light_metrics_cleaned = light_metrics_cleaned,
      SESOI = s
    )
    
    sesoi_sweep_all <- bind_rows(
      sesoi_sweep_all,
      tibble(
        metric = metric_name,
        group1 = group1,
        group2 = group2,
        SESOI = s,
        prop_outside_SESOI = res$prop_outside_SESOI,
        equivalence = res$is_equivalent
      )
    )
  }
}

print(sesoi_sweep_all)

# Create plots for each unique metric + group pair
plots <- sesoi_sweep_all %>%
  group_by(metric, group1, group2) %>%
  group_split() %>%
  map(\(df) {
    metric_name <- unique(df$metric)
    g1 <- unique(df$group1)
    g2 <- unique(df$group2)
    
    p <- ggplot(df, aes(x = SESOI, y = prop_outside_SESOI)) +
      geom_line() +
      geom_point() +
      labs(
        title = paste0("Proportion outside SESOI\nMetric: ", metric_name,
                       "\n", g1, " vs ", g2),
        x = "SESOI",
        y = "Proportion outside SESOI"
      ) +
      theme_bw()
    
    # Save if you want:
    ggsave(
      filename = paste0("prop_outside_", metric_name, "_", g1, "_vs_", g2, ".png"),
      plot = p,
      path = here::here("equivalence_test_analysis", "outputs", "sesoi"),
      width = 6,
      height = 4,
      dpi = 300,
      bg = "white"
    )
    
    p
  })

```

## Different visualisation: stacked bar plot of "equivalence classification results" based on SESOI choice
In other words, visualise of choice of SESOI affects results for each metric pairwise-comparisons.

### First: Calculation across metrics 
```{r}
# Turn equivalence col into factor
sesoi_sweep_all$equivalence <- as.factor(sesoi_sweep_all$equivalence)

# Calculate number of pairwise comparisons yielding to unique equivalence result
sesoi_sweep_all_proportions <- sesoi_sweep_all %>%
  group_by(SESOI, equivalence) %>%
  summarise(count = n(), .groups = "drop") 


ggplot(sesoi_sweep_all_proportions, aes(x = as.factor(SESOI), fill = equivalence)) +
  geom_bar(position = "fill") +
  labs(
    title = "Equivalence test results across pairwise-comparisons based on SESOI",
    x = "SESOI",
    y = "Proportion",
    fill = "Equivalence test outcome"
  ) +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal()

```

### Second, Calculation for each metric 
```{r}
# Calculate number of pairwise comparisons yielding to unique equivalence result
eq_results_indivmetric <- sesoi_sweep_all %>%
  group_by(SESOI, metric, equivalence) %>%
  summarise(count = n(), .groups = "drop") 

ggplot(eq_results_indivmetric, aes(x = as.factor(SESOI), fill = equivalence)) +
  geom_bar(position = "fill") +
  labs(
    title = "Equivalence test results across SESOI",
    x = "SESOI",
    y = "Proportion",
    fill = "Equivalence Result"
  ) +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal()

```


```{r}
# Apply multiple corrections
ttest_final <- ttest_all %>% 
  mutate(p.adj = stats::p.adjust(p,
                           "fdr", # we choose FDR (or BH - same thing) as method
                           n=length(p))) %>% # we want to correct for all comparisons, i.e. n=36
  rstatix::add_significance("p.adj") %>%
  dplyr::relocate(metric, .before= .y.) # re-order columns

# Rename some columns to eliminate the "_all" from the metric col
ttest_final <- ttest_final %>%
  mutate(metric = stringr::str_replace(metric, "_all$", ""))
```

## Turning this into a gt table

```{r}
ttest_table <- ttest_final %>%
  select(-c(.y., p.signif)) %>% #we do not need this col in the final summary
  gt::gt() %>%
  gt::tab_header(title = gt::md("**T-test results**")) %>% #Add title
  gt::cols_label(metric = "Metric",
             group1 = "Group 1",
             group2 = "Group 2",
             n1 = "N1",
             n2 = "N2",
             statistic = "Statistic",
             df = "Df",
             cohens_d = "Cohen's d",
             p.adj = "Adjusted p",
             p.adj.signif = "Significance") %>%
  gt::cols_align(align = "center",
                 columns = dplyr::everything()) %>%
  gt::tab_options(table.width=gt::pct(80)) #increasing table width so that it does not get cropped when saving
                
  
```

### Saving this gt object

```{r}
# Load chromote, needed to save using g
library(chromote)

# Make sure you set your path to Chrome using Sys.setenv(CHROMOTE_CHROME = "path/to/chrome.exe") and check this was correct by running chromote::find_chrome()

# Also, make sure to have a folder called "supplementary" in the "outputs" folder!
## The following line ensures that such folder exists
supp_dir <-  here::here("outputs", "supplementary")
if (!dir.exists(supp_dir)) dir.create(supp_dir, recursive = TRUE)


gt::gtsave(ttest_table,
           filename = "ttest_table.png",
           path = here::here("outputs", "supplementary"),
           vwidth = 1450,
           vheight = 800)
```

## Calculating mean and sd of all timing functions in order to report result of the t-test

```{r}
# Define a function that calculates the mean of each id metric
## Note that this function uses the function style_time, which is already imported in metrics_comparison.Rmd from the file vis_metrics_funs.R

metrics_means <- function(df, df_name) {
  
  # Compute means and sds
  df <- df %>%
    summarise(
      mean_ids_raw = style_time(mean(as.numeric(mean_raw), na.rm = TRUE)),
      sd_ids_raw = style_time(sd(as.numeric(mean_raw), na.rm = TRUE)),
      mean_ids_wrlg = style_time(mean(as.numeric(mean_wrlg), na.rm = TRUE)),
      sd_ids_wrlg = style_time(sd(as.numeric(mean_wrlg), na.rm = TRUE)),
      mean_ids_clusters = style_time(mean(as.numeric(mean_clusters), na.rm = TRUE)),
      sd_ids_clusters = style_time(sd(as.numeric(mean_clusters), na.rm = TRUE))
    )
  
  #Adding col to specify which metric and rename the df
  df$metric <- df_name
  
  return(df)
}

# Now we run a for loop on different metric dfs to calculate the mean 

# Create a named list of data frames
metrics_dfs <- list(
  mlit250_all = mlit250_all,
  mlit1000_all = mlit1000_all,
  llit10_all = llit10_all,
  llit250_all = llit250_all,
  llit1000_all = llit1000_all,
  flit10_all = flit10_all,
  flit250_all = flit250_all,
  flit1000_all = flit1000_all,
  tat1000_all = tat1000_all,
  tat250_all = tat250_all
)

# Initiate empty list to store results
means_metric_list <- list()

# Create for loop 
for (df_name in names(metrics_dfs)) {
 
   df <- metrics_dfs[[df_name]]
  
  # Compute the means for the current data frame
  df_means <- metrics_means(df, df_name)
  
  # Store the resulting data frame in the list, using the index i
  means_metric_list[[df_name]] <- df_means
}

# Convert the list of data frames to a single data frame
means_metrics_df <- bind_rows(means_metric_list)
```

## Calculating mean and sd of is and iv

```{r}
is_means <- is_all %>%
    summarise(
      mean_ids_raw = mean(IS_raw, na.rm = TRUE),
      sd_ids_raw = sd(IS_raw, na.rm = TRUE),
      mean_ids_wrlg = mean(IS_wrlg, na.rm = TRUE),
      sd_ids_wrlg = sd(IS_wrlg, na.rm = TRUE),
      mean_ids_clusters = mean(IS_clusters, na.rm = TRUE),
      sd_ids_clusters = sd(IS_clusters, na.rm = TRUE),
    ) %>%
  mutate(metric = "is")

iv_means <- iv_all %>%
  summarise(
      mean_ids_raw = mean(IV_raw, na.rm = TRUE),
      sd_ids_raw = sd(IV_raw, na.rm = TRUE),
      mean_ids_wrlg = mean(IV_wrlg, na.rm = TRUE),
      sd_ids_wrlg = sd(IV_wrlg, na.rm = TRUE),
      mean_ids_clusters = mean(IV_clusters, na.rm = TRUE),
      sd_ids_clusters = sd(IV_clusters, na.rm = TRUE),
    ) %>%
  mutate(metric = "iv")

```

## Now we would like to have a single table for all metrics.

The issue with this is that bind_rows() will not merge cols that are of different data types (hms and numeric, in our case). Hence, we first convert everything to character, and then combine. We can do this since we are creating this table for display only, and not for any calculations.

```{r}
# Convert means_metrics_df cols to character
means_metrics_df <- means_metrics_df %>%
  mutate(dplyr::across(.fns = as.character))

# Convert numeric cols to character and round to 4 digits
iv_means <- iv_means %>%
  mutate(dplyr::across(where(is.numeric), ~ format(.x, digits = 4)))

# Convert numeric cols to character and round to 4 digits
is_means <- is_means %>%
  mutate(dplyr::across(where(is.numeric), ~ format(.x, digits = 4)))


# Combine the data using bind_rows
means_dfs <- dplyr::bind_rows(means_metrics_df, is_means, iv_means)

# Rename some columns to eliminate the "_all" from the metric col
means_dfs <- means_dfs %>%
  mutate(metric = stringr::str_replace(metric, "_all$", "")) %>%
  dplyr::relocate(metric, .before= mean_ids_raw) # re-order columns so that metric is first one
```

### Create gt table for this dataframe

```{r}
means_table <- means_dfs %>%
  gt::gt() %>%
  gt::tab_header(title = gt::md("**Means and SDs of light exposure metrics (n=12)**")) %>% #Add title
  gt::cols_label(metric = "Metric",
             mean_ids_raw = "Raw dataset (mean)",
             sd_ids_raw = "Raw dataset (SD)",
             mean_ids_wrlg = "Clean (Wear log) dataset (mean)",
             sd_ids_wrlg = "Clean (Wear log) dataset (SD)",
             mean_ids_clusters = "Clean (algorithm) dataset (mean)",
             sd_ids_clusters = "Clean (algorithm) dataset (SD)") %>%
  gt::cols_align(align = "center",
                 columns = dplyr::everything()) 

# Save the gt table
## Make sure you set your path to Chrome using Sys.setenv(CHROMOTE_CHROME = "path/to/chrome.exe") and check this was correct by running chromote::find_chrome()

# AGain, if not already there: make sure you have a folder called "supplementary" in the "outputs" folder!
## The following line ensures that such folder exists
supp_dir <-  here::here("outputs", "supplementary")
if (!dir.exists(supp_dir)) dir.create(supp_dir, recursive = TRUE)

gt::gtsave(means_table,
           filename = "means_table.png",
           path = here::here("outputs", "supplementary"))


```

## Calculating the unstandardised effect size for the significant result, as we need to report this in the paper

```{r}
tat250_unst_effsize <- tat250_all %>%
  summarise(mean = mean(delta_clusters))
```
