---
title: "metrics_stats"
author: "Carolina Guidolin"
date: "2024-12-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Aim

To calculate differences between metrics calculated in the script 06_metrics_comparison.Rmd. Specifically, we will be using a t-test to do this.

## Let's first check the normality of our data

### Import the norm_check functions before running this as well as necessary packages

```{r}
# Function to check normality
base::source(here::here("03_analysis", "functions", "norm_check_funs.R"))

library(ggpubr)
library(rstatix)
```

### Testing for normality of residuals

```{r}
options(scipen = 999) # avoid scientific notation 
set.seed(20250709)

# Create list of data frames containing metric values
light_metrics_list <- list(
  mlit250_all = mlit250_all,
  mlit1000_all = mlit1000_all,
  llit10_all = llit10_all,
  llit250_all = llit250_all,
  llit1000_all = llit1000_all,
  flit10_all = flit10_all,
  flit250_all = flit250_all,
  flit1000_all = flit1000_all,
  tat1000_all = tat1000_all,
  tat250_all = tat250_all,
  mean_all = mean_all,
  m10_all = m10_all,
  iv_all = iv_all,
  is_all = is_all
)

# Visualise normality of the means for each df
for (i in seq_along(light_metrics_list)) {
  df <- light_metrics_list[[i]]
  df_name <- names(light_metrics_list)[i]
  print(norm_check_means(df, df_name = df_name))
}


## The distribution does not look normal, and for several metrics the Shapiro Wilk test also returns significant results
## Hence, non-parametric alternative is needed

```

### Performing the non-parametric Friedman test for the metrics, and calculate pairwise comparisons (post-hoc tests) using Wilcoxon test

#### First, we create a function to perform the two test for all metrics

```{r}
# Friedman test
## First, we create a function for the metrics that have been calculated as average of 6 participation days
## These are: mlit250, mlit1000, llit10, llit250, llit1000, flit10, flit250, flit1000, tat1000, tat250, mean (daytime, M10m)

friedman_and_posthoc <- function(df, pval_threshold = 0.05, metric_name = NULL) {
  
  # Get the name of the input df as a string
    if (is.null(metric_name)) {
    metric_name <- deparse(substitute(df))  
  }
  
  # Turn into long format
  df_long <- df %>%
 # Drop any delta cols and sd cols if they exist
    select(-any_of(c("delta_wrlg", "delta_clusters", "sd_raw", "sd_wrlg", "sd_clusters"))) %>%
    # Automatically select *_raw, *_wrlg, *_clusters
    pivot_longer(
      cols = matches("(_raw|_wrlg|_clusters)$"),
      names_to = "dataset",
      values_to = "value"
    ) %>%
    mutate(value = as.numeric(value),
           Id = as.factor(Id)) %>%
    ungroup()
  
  summary(df_long$value)
  
  # Friedman test and effect size (Kendall's W)
  friedman <- df_long %>%
    rstatix::friedman_test(value ~ dataset | Id) %>%
    mutate(
      kendallsw = rstatix::friedman_effsize(df_long, value ~ dataset | Id)$effsize,
      metric = metric_name
    )
  
  # Post-hoc only if friedman test is significant
  if (friedman$p < pval_threshold) {
    # Perform Wilcoxon pairwise comparisons
    posthoc <- df_long %>%
      rstatix::pairwise_wilcox_test(
        value ~ dataset,
        paired = TRUE,
        p.adjust.method = "fdr"
      ) %>%
      # Add metric name
      mutate(
        metric = metric_name # add metric name
    )
  } 
  else {
  posthoc <- NULL
  }
  
return(list(
    friedman = friedman,
    posthoc = posthoc
  ))
  
}

```

### Apply Friedman testing function to all metrics dfs

```{r}
library(purrr)

# Use list of light metrics indicated above

results <- lapply(names(light_metrics_list), function(nm) {
  fried_res <- friedman_and_posthoc(light_metrics_list[[nm]], 0.05, nm)
  return(fried_res)
})

# Save results in dfs
friedman_all <- purrr::map_dfr(results, "friedman")
posthoc_all  <- purrr::map_dfr(results, "posthoc") 
```

### Performing equivalence testing

The main idea of performing equivalence testing here is to compare the metrics across datasets and identify whether the difference is small enough to treat the three datasets as practically the same. We perform equivalence testing using bootstrapping. Bootstrapping is a way to get an empirical distribution of possible effect sizes (as if we repeated the study many times), i.e. estimating the sampling distribution of our effect size. So put simply, bootstrapping gives us a way to answer: How consistent (robust) is my evidence that this difference is small enough to be ignorable?

### Applying bootstrap eq testing

```{r}
run_bootstrap_equiv_test <- function(metric_name, group1, group2, light_metrics_list,
                                     n_bootstraps = 10^4, SESOI = NULL, alpha = 0.05) {
  
  # Pull the original data
  df <- light_metrics_list[[metric_name]]
  if (is.null(df)) stop(paste0("Metric '", metric_name, "' not found!"))
  
  # Add 'mean_' prefix if needed (is and iv)
  special_metrics <- c("iv_all", "is_all")  # these don't have mean_* cols
  if (!(metric_name %in% special_metrics)) {
    if (!(group1 %in% names(df))) group1 <- paste0("mean_", group1)
    if (!(group2 %in% names(df))) group2 <- paste0("mean_", group2)
  }
  
  data1 <- as.numeric(df[[group1]])
  data2 <- as.numeric(df[[group2]])
  
  # Checking if cols are correctly identified 
  if (any(is.null(c(data1, data2)))) {
    return(tibble::tibble(
      error = paste0("Group columns not found: ", group1, ", ", group2),
      cohens_d = NA_real_,
      lower_CI = NA_real_,
      upper_CI = NA_real_,
      is_equivalent = NA,
      prop_inside_SESOI = NA_real_
    ))
  }
  
  # Check for missing pairs
  missing_pairs <- sum(is.na(data1) | is.na(data2))
  total_pairs <- length(data1)
  
  if (missing_pairs > 0) {
    return(tibble::tibble(
      error = paste0("Data contains ", missing_pairs, " missing pairs out of ", total_pairs),
      cohens_d = NA_real_,
      lower_CI = NA_real_,
      upper_CI = NA_real_,
      is_equivalent = NA,
      prop_inside_SESOI = NA_real_
    ))
  }
  
  # Calculate effect size and bootstraps
  effect.size <- effectsize::cohens_d(data1, data2)$Cohens_d
  bootstraps <- bootstrap_differences(n_bootstraps, data1, data2)
  
  # Filtering non-NA bootstraps 
  bootstraps <- bootstraps[!is.na(bootstraps)]
  
  if (length(bootstraps) == 0) {
    return(tibble::tibble(
      error = "All bootstrap samples are NA",
      cohens_d = effect.size,
      lower_CI = NA_real_,
      upper_CI = NA_real_,
      is_equivalent = NA,
      prop_inside_SESOI = NA_real_
    ))
  }
  
  # Visualise the bootstraps
  ci <- quantile(bootstraps, probs = c(0.05, 0.95))
  lower_CI <- ci[1]
  upper_CI <- ci[2]
  
  plot_obj <- tibble::tibble(value = bootstraps) |>
  ggplot(aes(x = value)) +
  geom_histogram(binwidth = 0.02,
                 aes(fill = value >= SESOI | value <= -SESOI)) +
  coord_cartesian(xlim = c(-1, 1)) +
  labs(
    title = paste0("Bootstrap distribution for ", metric_name, ": ", group1, " vs ", group2),
    subtitle = paste0("Smallest effect size of interest (SESOI):", SESOI),
    x = "Effect size",
    fill = "Effect larger than SESOI"
  ) +
  # Add SESOI region, CIs and observed eff size 
  geom_vline(
    aes(xintercept = effect.size, color = "Observed Cohen's d"),
  linetype = "dashed",
  linewidth = 1
  ) +
  geom_vline(
    aes(xintercept = lower_CI, color = "CI bounds"),
  linetype = "dashed",
  linewidth = 1
  ) +
  geom_vline(
    aes(xintercept = upper_CI, color = "CI bounds"),
    linetype = "dashed",
    linewidth = 1
  ) +
  # Shaded SESOI region
  annotate(
    "rect",
    xmin = -SESOI, xmax = SESOI,
    ymin = 0, ymax = Inf,
    fill = "grey", alpha = 0.2
  ) +
  scale_color_manual(
    values = c(
      "Observed Cohen's d" = "red",
      "CI bounds" = "blue"
    )
  ) +
  theme_minimal()

  print(plot_obj)
  
  # Calculating confidence intervals 
  ci <- quantile(bootstraps, probs = c(alpha, 1 - alpha), na.rm = TRUE)
  is_equiv <- (ci[1] > -SESOI) && (ci[2] < SESOI)
  prop_inside <- mean(bootstraps > -SESOI & bootstraps < SESOI, na.rm = TRUE)
  
return(list(
  result_table = tibble::tibble(
    cohens_d = effect.size,
    lower_CI = ci[1],
    upper_CI = ci[2],
    is_equivalent = is_equiv,
    prop_inside_SESOI = prop_inside
  ),
  plot = plot_obj  
))
}

```

## Try out on one metric only

```{r}
# Pick random metric manually 
# Pick one example
run_bootstrap_equiv_test(
  metric_name = "llit10_all",
  group1 = "clusters",
  group2 = "wrlg",
  SESOI = 0.4
)

```

## Try out on all pairwise combinations from the post-hoc tests

```{r}
# Adding equivalence to post-hoc results 
equiv_results <- posthoc_all %>%
  mutate(equiv_result = pmap(
    list(metric, group1, group2),
    # For each row, run bootrasp eq test on paurs of groups for given metric using original data in metrics_list 
    ~ run_bootstrap_equiv_test(..1, ..2, ..3, SESOI = 0.3, light_metrics_list)
  )) %>%
  tidyr::unnest(equiv_result)

```
## Only saving the plots
```{r}
for (metric_name in names(light_metrics_list)) {
  result <- run_bootstrap_equiv_test(
    metric_name, group1, group2, light_metrics_list, SESOI = 0.3
  )
  # Save plot with dynamic name
  ggsave(
    paste0("bootstrap_", metric_name, ".png"),
    plot = result$plot_obj, 
    path = here::here("equivalence_test_analysis", "outputs"),
    width = 8,
    height = 5,
    dpi = 300
  )
}

```




```{r}
# Apply multiple corrections
ttest_final <- ttest_all %>% 
  mutate(p.adj = stats::p.adjust(p,
                           "fdr", # we choose FDR (or BH - same thing) as method
                           n=length(p))) %>% # we want to correct for all comparisons, i.e. n=36
  rstatix::add_significance("p.adj") %>%
  dplyr::relocate(metric, .before= .y.) # re-order columns

# Rename some columns to eliminate the "_all" from the metric col
ttest_final <- ttest_final %>%
  mutate(metric = stringr::str_replace(metric, "_all$", ""))
```

## Turning this into a gt table

```{r}
ttest_table <- ttest_final %>%
  select(-c(.y., p.signif)) %>% #we do not need this col in the final summary
  gt::gt() %>%
  gt::tab_header(title = gt::md("**T-test results**")) %>% #Add title
  gt::cols_label(metric = "Metric",
             group1 = "Group 1",
             group2 = "Group 2",
             n1 = "N1",
             n2 = "N2",
             statistic = "Statistic",
             df = "Df",
             cohens_d = "Cohen's d",
             p.adj = "Adjusted p",
             p.adj.signif = "Significance") %>%
  gt::cols_align(align = "center",
                 columns = dplyr::everything()) %>%
  gt::tab_options(table.width=gt::pct(80)) #increasing table width so that it does not get cropped when saving
                
  
```

### Saving this gt object

```{r}
# Load chromote, needed to save using g
library(chromote)

# Make sure you set your path to Chrome using Sys.setenv(CHROMOTE_CHROME = "path/to/chrome.exe") and check this was correct by running chromote::find_chrome()

# Also, make sure to have a folder called "supplementary" in the "outputs" folder!
## The following line ensures that such folder exists
supp_dir <-  here::here("outputs", "supplementary")
if (!dir.exists(supp_dir)) dir.create(supp_dir, recursive = TRUE)


gt::gtsave(ttest_table,
           filename = "ttest_table.png",
           path = here::here("outputs", "supplementary"),
           vwidth = 1450,
           vheight = 800)
```

## Calculating mean and sd of all timing functions in order to report result of the t-test

```{r}
# Define a function that calculates the mean of each id metric
## Note that this function uses the function style_time, which is already imported in metrics_comparison.Rmd from the file vis_metrics_funs.R

metrics_means <- function(df, df_name) {
  
  # Compute means and sds
  df <- df %>%
    summarise(
      mean_ids_raw = style_time(mean(as.numeric(mean_raw), na.rm = TRUE)),
      sd_ids_raw = style_time(sd(as.numeric(mean_raw), na.rm = TRUE)),
      mean_ids_wrlg = style_time(mean(as.numeric(mean_wrlg), na.rm = TRUE)),
      sd_ids_wrlg = style_time(sd(as.numeric(mean_wrlg), na.rm = TRUE)),
      mean_ids_clusters = style_time(mean(as.numeric(mean_clusters), na.rm = TRUE)),
      sd_ids_clusters = style_time(sd(as.numeric(mean_clusters), na.rm = TRUE))
    )
  
  #Adding col to specify which metric and rename the df
  df$metric <- df_name
  
  return(df)
}

# Now we run a for loop on different metric dfs to calculate the mean 

# Create a named list of data frames
metrics_dfs <- list(
  mlit250_all = mlit250_all,
  mlit1000_all = mlit1000_all,
  llit10_all = llit10_all,
  llit250_all = llit250_all,
  llit1000_all = llit1000_all,
  flit10_all = flit10_all,
  flit250_all = flit250_all,
  flit1000_all = flit1000_all,
  tat1000_all = tat1000_all,
  tat250_all = tat250_all
)

# Initiate empty list to store results
means_metric_list <- list()

# Create for loop 
for (df_name in names(metrics_dfs)) {
 
   df <- metrics_dfs[[df_name]]
  
  # Compute the means for the current data frame
  df_means <- metrics_means(df, df_name)
  
  # Store the resulting data frame in the list, using the index i
  means_metric_list[[df_name]] <- df_means
}

# Convert the list of data frames to a single data frame
means_metrics_df <- bind_rows(means_metric_list)
```

## Calculating mean and sd of is and iv

```{r}
is_means <- is_all %>%
    summarise(
      mean_ids_raw = mean(IS_raw, na.rm = TRUE),
      sd_ids_raw = sd(IS_raw, na.rm = TRUE),
      mean_ids_wrlg = mean(IS_wrlg, na.rm = TRUE),
      sd_ids_wrlg = sd(IS_wrlg, na.rm = TRUE),
      mean_ids_clusters = mean(IS_clusters, na.rm = TRUE),
      sd_ids_clusters = sd(IS_clusters, na.rm = TRUE),
    ) %>%
  mutate(metric = "is")

iv_means <- iv_all %>%
  summarise(
      mean_ids_raw = mean(IV_raw, na.rm = TRUE),
      sd_ids_raw = sd(IV_raw, na.rm = TRUE),
      mean_ids_wrlg = mean(IV_wrlg, na.rm = TRUE),
      sd_ids_wrlg = sd(IV_wrlg, na.rm = TRUE),
      mean_ids_clusters = mean(IV_clusters, na.rm = TRUE),
      sd_ids_clusters = sd(IV_clusters, na.rm = TRUE),
    ) %>%
  mutate(metric = "iv")

```

## Now we would like to have a single table for all metrics.

The issue with this is that bind_rows() will not merge cols that are of different data types (hms and numeric, in our case). Hence, we first convert everything to character, and then combine. We can do this since we are creating this table for display only, and not for any calculations.

```{r}
# Convert means_metrics_df cols to character
means_metrics_df <- means_metrics_df %>%
  mutate(dplyr::across(.fns = as.character))

# Convert numeric cols to character and round to 4 digits
iv_means <- iv_means %>%
  mutate(dplyr::across(where(is.numeric), ~ format(.x, digits = 4)))

# Convert numeric cols to character and round to 4 digits
is_means <- is_means %>%
  mutate(dplyr::across(where(is.numeric), ~ format(.x, digits = 4)))


# Combine the data using bind_rows
means_dfs <- dplyr::bind_rows(means_metrics_df, is_means, iv_means)

# Rename some columns to eliminate the "_all" from the metric col
means_dfs <- means_dfs %>%
  mutate(metric = stringr::str_replace(metric, "_all$", "")) %>%
  dplyr::relocate(metric, .before= mean_ids_raw) # re-order columns so that metric is first one
```

### Create gt table for this dataframe

```{r}
means_table <- means_dfs %>%
  gt::gt() %>%
  gt::tab_header(title = gt::md("**Means and SDs of light exposure metrics (n=12)**")) %>% #Add title
  gt::cols_label(metric = "Metric",
             mean_ids_raw = "Raw dataset (mean)",
             sd_ids_raw = "Raw dataset (SD)",
             mean_ids_wrlg = "Clean (Wear log) dataset (mean)",
             sd_ids_wrlg = "Clean (Wear log) dataset (SD)",
             mean_ids_clusters = "Clean (algorithm) dataset (mean)",
             sd_ids_clusters = "Clean (algorithm) dataset (SD)") %>%
  gt::cols_align(align = "center",
                 columns = dplyr::everything()) 

# Save the gt table
## Make sure you set your path to Chrome using Sys.setenv(CHROMOTE_CHROME = "path/to/chrome.exe") and check this was correct by running chromote::find_chrome()

# AGain, if not already there: make sure you have a folder called "supplementary" in the "outputs" folder!
## The following line ensures that such folder exists
supp_dir <-  here::here("outputs", "supplementary")
if (!dir.exists(supp_dir)) dir.create(supp_dir, recursive = TRUE)

gt::gtsave(means_table,
           filename = "means_table.png",
           path = here::here("outputs", "supplementary"))


```

## Calculating the unstandardised effect size for the significant result, as we need to report this in the paper

```{r}
tat250_unst_effsize <- tat250_all %>%
  summarise(mean = mean(delta_clusters))
```
